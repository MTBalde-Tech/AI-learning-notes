{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "373d2def",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ“Š Notebook 4 â€“ Logistic Regression Basics (Binary Classification)\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "- Create a simple **binary classification** dataset  \n",
    "- Use the **sigmoid function**  \n",
    "- Define the **logistic regression model**  \n",
    "- Implement the **binary cross-entropy cost function**  \n",
    "- Compute **gradients**  \n",
    "- Train the model using **gradient descent**  \n",
    "- Visualize predictions and the decision boundary  \n",
    "\n",
    "Logistic regression is a fundamental algorithm for classification in machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f296b1",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Simple Binary Classification Dataset\n",
    "\n",
    "We will create a small 1D dataset.\n",
    "\n",
    "- Input `x` is a numeric feature  \n",
    "- Output `y` is a class label: 0 or 1  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a682fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simple 1D dataset\n",
    "x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=float)\n",
    "y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1], dtype=float)  # class labels: 0 or 1\n",
    "\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y (class)\")\n",
    "plt.title(\"Simple Binary Classification Dataset\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c887a5d",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Sigmoid Function\n",
    "\n",
    "The logistic regression model uses the **sigmoid** (logistic) function:\n",
    "\n",
    "\\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\]\n",
    "\n",
    "This function maps any real number to the range (0, 1), which can be interpreted as a probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb967b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "z = np.linspace(-10, 10, 400)\n",
    "s = sigmoid(z)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(z, s)\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"Ïƒ(z)\")\n",
    "plt.title(\"Sigmoid Function\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d922c51",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Logistic Regression Model\n",
    "\n",
    "For a single feature `x`, the logistic regression model is:\n",
    "\n",
    "\\[ \\hat{y} = \\sigma(wx + b) \\]\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( w \\) is the weight  \n",
    "- \\( b \\) is the bias  \n",
    "- \\( \\hat{y} \\) is the predicted probability that the class is 1  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913d0849",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_prob(x, w, b):\n",
    "    z = w * x + b\n",
    "    return sigmoid(z)\n",
    "\n",
    "# Example with w = 0, b = 0\n",
    "w_test = 0.0\n",
    "b_test = 0.0\n",
    "y_hat_example = predict_prob(x, w_test, b_test)\n",
    "print(\"Predicted probabilities with w=0, b=0:\")\n",
    "print(y_hat_example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7af65d",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Cost Function â€“ Binary Cross-Entropy\n",
    "\n",
    "The cost function for logistic regression is the **binary cross-entropy**:\n",
    "\n",
    "\\[ J(w, b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right] \\]\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( m \\) is the number of training examples  \n",
    "- \\( y^{(i)} \\) is the true label (0 or 1)  \n",
    "- \\( \\hat{y}^{(i)} \\) is the predicted probability  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b75016",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_cost(x, y, w, b):\n",
    "    m = len(x)\n",
    "    y_hat = predict_prob(x, w, b)\n",
    "    # Avoid log(0) by clipping\n",
    "    epsilon = 1e-15\n",
    "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "    cost = -(1 / m) * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "    return cost\n",
    "\n",
    "cost_initial = compute_cost(x, y, 0.0, 0.0)\n",
    "print(\"Initial cost with w=0, b=0:\", cost_initial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318bf8f3",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Gradients of the Cost Function\n",
    "\n",
    "The gradients of the cost with respect to \\( w \\) and \\( b \\) are:\n",
    "\n",
    "\\[ \\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)}) x^{(i)} \\]\n",
    "\n",
    "\\[ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)}) \\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e942dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_gradients(x, y, w, b):\n",
    "    m = len(x)\n",
    "    y_hat = predict_prob(x, w, b)\n",
    "    error = y_hat - y\n",
    "    dw = (1 / m) * np.sum(error * x)\n",
    "    db = (1 / m) * np.sum(error)\n",
    "    return dw, db\n",
    "\n",
    "dw_initial, db_initial = compute_gradients(x, y, 0.0, 0.0)\n",
    "print(\"Initial dw:\", dw_initial)\n",
    "print(\"Initial db:\", db_initial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefab84a",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Gradient Descent for Logistic Regression\n",
    "\n",
    "We update \\( w \\) and \\( b \\) using gradient descent:\n",
    "\n",
    "\\[ w := w - \\alpha \\frac{\\partial J}{\\partial w} \\]  \n",
    "\\[ b := b - \\alpha \\frac{\\partial J}{\\partial b} \\]\n",
    "\n",
    "Where \\( \\alpha \\) is the learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207057e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradient_descent(x, y, w_init, b_init, learning_rate, num_iterations):\n",
    "    w = w_init\n",
    "    b = b_init\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        dw, db = compute_gradients(x, y, w, b)\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "\n",
    "        cost = compute_cost(x, y, w, b)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Iteration {i:3d}: w = {w:.4f}, b = {b:.4f}, cost = {cost:.4f}\")\n",
    "\n",
    "    return w, b, cost_history\n",
    "\n",
    "w_init = 0.0\n",
    "b_init = 0.0\n",
    "learning_rate = 0.1\n",
    "num_iterations = 501\n",
    "\n",
    "w_final, b_final, cost_history = gradient_descent(x, y, w_init, b_init, learning_rate, num_iterations)\n",
    "\n",
    "print(\"\\nFinal parameters:\")\n",
    "print(\"w =\", w_final)\n",
    "print(\"b =\", b_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697771a1",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Cost Over Iterations\n",
    "\n",
    "Let's visualize how the cost decreases during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dd425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iterations = np.arange(len(cost_history))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(iterations, cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost J(w, b)\")\n",
    "plt.title(\"Cost Decrease Over Iterations (Logistic Regression)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8704d40e",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Visualizing Predictions\n",
    "\n",
    "Now we plot:\n",
    "\n",
    "- The original data points  \n",
    "- The predicted probability curve  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae012f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_dense = np.linspace(min(x), max(x), 200)\n",
    "y_prob = predict_prob(x_dense, w_final, b_final)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y, label=\"Data (0 or 1)\")\n",
    "plt.plot(x_dense, y_prob, label=\"Predicted probability\", linewidth=2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y / probability\")\n",
    "plt.title(\"Logistic Regression â€“ Data and Predicted Probabilities\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6e3e04",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Decision Boundary\n",
    "\n",
    "The **decision boundary** is the value of x where the model predicts probability 0.5:\n",
    "\n",
    "\\[ \\hat{y} = 0.5 \\Rightarrow wx + b = 0 \\Rightarrow x = -\\frac{b}{w} \\]\n",
    "\n",
    "Values of x below this boundary are classified as 0, above as 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba62b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if w_final != 0:\n",
    "    decision_boundary = -b_final / w_final\n",
    "    print(\"Decision boundary (x where probability = 0.5):\", decision_boundary)\n",
    "else:\n",
    "    decision_boundary = None\n",
    "    print(\"Decision boundary undefined because w_final = 0\")\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y, label=\"Data (0 or 1)\")\n",
    "plt.plot(x_dense, y_prob, label=\"Predicted probability\", linewidth=2)\n",
    "if decision_boundary is not None:\n",
    "    plt.axvline(decision_boundary, color=\"red\", linestyle=\"--\", label=\"Decision boundary\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y / probability\")\n",
    "plt.title(\"Logistic Regression â€“ Decision Boundary\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5811aa",
   "metadata": {},
   "source": [
    "\n",
    "## âœ… Summary\n",
    "\n",
    "In this notebook, you:\n",
    "\n",
    "- Built a simple binary classification dataset  \n",
    "- Defined and used the sigmoid function  \n",
    "- Implemented logistic regression: \\( \\hat{y} = \\sigma(wx + b) \\)  \n",
    "- Implemented the binary cross-entropy cost function  \n",
    "- Computed gradients of the cost  \n",
    "- Trained the model with gradient descent  \n",
    "- Visualized cost decrease and predicted probabilities  \n",
    "- Found the decision boundary \\( x = -b / w \\)  \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
